<html>

<head>
    <title>
        Mr.Fu's Page
    </title>
</head>

<body>
    <h1>Machine Learning Series</h1>
    <h2>Math</h2>
    <div>Some good online math materials.</div>
    <ul>
        <h3>Videos</h3>
        <ul>
            <li><a href="https://www.youtube.com/playlist?list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF">Machine Learning -
                    StatQuest with Josh Starmer</a></li>
        </ul>
    </ul>

    <h1>Factor Analysis</h1>
    <div>Can do parameter reduction.</div>
    <ul>
        <h3>Videos</h3>
        <ul>
            <li><a href="https://www.youtube.com/watch?v=lJ0cXPoEozg">
                Factor Analysis and Probabilistic PCA
            </a></li>
        </ul>
    </ul>

    <h1>Loss Function</h1>
    <h2>xxx</h2>
    <div>xxx</div>
    <ul>
        <h3>Cross Entropy</h3>
        <ul>
            <li><a href="https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8-%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-%E5%9F%BA%E7%A4%8E%E4%BB%8B%E7%B4%B9-%E6%90%8D%E5%A4%B1%E5%87%BD%E6%95%B8-loss-function-2dcac5ebb6cb">機器/深度學習: 基礎介紹-損失函數(loss function)</a></li>
            <li><a href="https://ithelp.ithome.com.tw/articles/10218158">【Day 20】 Google ML - Lesson 6 - 使用損失函數(Loss Functions)來評估ML模型的好壞吧! MSE, RMSE, Cross Entropy的計算方法與特性</a></li>
            <li><a href="https://www.youtube.com/watch?v=6ArSys5qHAU&list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1&index=11">StatQueset: Neural Networks Part 6: Cross Entropy</a></li>
            <li><a href="https://www.youtube.com/watch?v=xBEh66V9gZo&list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1&index=12">StatQuest精彩的Cross Entropy計算案例</a></li>
            <li><a href="https://www.cupoy.com/qa/club/ai_tw/0000016D6BA22D97000000016375706F795F72656C656173654B5741535354434C5542/0000017BA0B0F427000000086375706F795F72656C656173655155455354">Cross Entroy的實際運算案例</a></li>
            <li><a href="https://towardsdatascience.com/cross-entropy-negative-log-likelihood-and-all-that-jazz-47a95bd2e81">Cross-Entropy, Negative Log-Likelihood, and All That Jazz</a></li>
        </ul>
    </ul>    


    <h1>VAE</h1>
    <h2>Variational Autoencoder</h2>
    <div>xxx</div>
    <ul>
        <h3>Webpage</h3>
        <ul>
            <li><a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/">Tutorial - What is a variational autoencoder?</a></li>
            <li><a href="https://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/">Generating Large Images from Latent Vectors</a></li>
        </ul>
    </ul>


    <h1>Probability</h1>
    <h2>Maximum Likelihood Estimation (MLE)</h2>
    <div>MLE is an estimation for how likely the shape of your PDF fits your given data.</div>
    <ul>
        <h3>Videos</h3>
        <ul>
            <li><a href="https://www.youtube.com/watch?v=XepXtl9YKwc">Maximum Likelihood, clearly explained!!! </li>
            <li><a href="https://www.youtube.com/playlist?list=PLdxWrq0zBgPXWD7_sc7uf8BdirORFRA2-">Maximum Likelihood
                    Estimation </a></li>
        </ul>
        <h3>Plotter</h3>
        <ul>
            <li><a href="https://homepage.divms.uiowa.edu/~mbognar/applets/bin.html">Binomial Distribution Graph</a>
            </li>
            <li><a href="https://keisan.casio.com/menu/system/000000000540">Keisan Online Calculator</a></li>
        </ul>
    </ul>

    <h2>KL Divergence</h2>
    <div>xxxxx</div>
    <ul>
        <h3>Videos</h3>
        <ul>
            <li><a href="https://www.youtube.com/watch?v=SxGYPqCgJWM">Intuitively Understanding the KL Divergence
            </li>
        </ul>
        <h3>Plotter</h3>
        <ul>
            <li>xxx</a>
            </li>
        </ul>
    </ul>
</body>

</html>